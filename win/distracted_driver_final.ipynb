{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像预处理部分\n",
    "利用os和opencv导入图片库，并对其进行resize，使符合网络需求。\n",
    "1.导入imgs.zip中的图片以及driver_imgs_list.csv\n",
    "#listdir的参数是文件夹的路径\n",
    "#resize可以缩放图片可参考如下\n",
    "http://www.xuebuyuan.com/1971769.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.applications import inception_v3,xception,inception_resnet_v2\n",
    "from keras.applications import InceptionV3,Xception,InceptionResNetV2\n",
    "from keras.callbacks import Callback,ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "import keras.backend as kb\n",
    "from keras.optimizers import SGD,Adam\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data transfer is finished\n",
      "19407 3017 79726\n"
     ]
    }
   ],
   "source": [
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "def transfer_image(driver,targetpath):\n",
    "    path = \"./imgs/train\"\n",
    "    images = datacsv.loc[(datacsv['subject']==driver),['img']]\n",
    "    classnames = datacsv.loc[(datacsv['subject']==driver),['classname']] \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        resource = \"\".join(os.path.join(path,classnames.values[i][0],images.values[i][0]))\n",
    "        target = \"\".join(os.path.join(targetpath,classnames.values[i][0],images.values[i][0]))\n",
    "        shutil.copy(resource,target)\n",
    "    return len(images)\n",
    "\n",
    "# 根据驾驶员分割训练集和验证集\n",
    "x_drivers = []\n",
    "y_drivers = []\n",
    "datacsv = pd.DataFrame(pd.read_csv('driver_imgs_list.csv'))\n",
    "x_drivers = datacsv['subject'].unique()[:22]\n",
    "y_drivers = datacsv['subject'].unique()[22:]\n",
    "\n",
    "# 重新建立目标文件树\n",
    "rmrf_mkdir(\"./data\")\n",
    "print('/data/ recreate') \n",
    "os.makedirs(\"./data/test/val\")\n",
    "for c in [\"c0\",\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\",\"c7\",\"c8\",\"c9\"]: \n",
    "    os.makedirs(\"./data/train/{0}\".format(c))\n",
    "    os.makedirs(\"./data/validation/{0}\".format(c))\n",
    "print('folders created !') \n",
    "\n",
    "# 复制文件到目标文件夹下\n",
    "tra_nb = 0\n",
    "val_nb = 0\n",
    "for driver in x_drivers:\n",
    "    tra_nb += transfer_image(driver, targetpath=\"./data/train\")\n",
    "print (\"train_data transfer is finished\")\n",
    "for driver in y_drivers:\n",
    "    val_nb += transfer_image(driver,targetpath=\"./data/validation/\")\n",
    "print (\"validation_data transfer is finished\")\n",
    "for file in os.listdir(\"./imgs/test/\"):\n",
    "    resource = \"\".join(os.path.join(\"./imgs/test/\",file))\n",
    "    target = \"\".join(os.path.join(\"./data/test/val\",file))\n",
    "    shutil.copy(resource,target)\n",
    "tex_nb = len(os.listdir(\"./data/test/val/\"))\n",
    "print (\"test_data transfer is finished\")\n",
    "print (tra_nb, val_nb, tex_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch,model):\n",
    "    a = epoch % 4\n",
    "    lr = 0.0001\n",
    "    if a == 0:\n",
    "        lr = lr / 10\n",
    "    elif a == 3:\n",
    "        lr = lr\n",
    "    else:\n",
    "        lr = lr / 100\n",
    "    print (\"learning rate is %s\"%lr)\n",
    "    return lr\n",
    "\n",
    "def Model_fit(size,MODEL,preprocess_fun,optimizer,History,tune=0,Epochs=20,drop=0.3):\n",
    "    width = size[0]\n",
    "    height = size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = Lambda(preprocess_fun)(input_tensor)\n",
    "        \n",
    "    base_model = MODEL(input_tensor=x, include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "    new_output = Dropout(drop)(base_model.output)\n",
    "    new_output = Dense(10, activation=\"softmax\")(new_output)\n",
    "    new_model = Model(inputs=base_model.input, outputs=new_output)\n",
    "    if tune:\n",
    "        new_model.load_weights(\"./a_%s.hdf5\"%MODEL.__name__)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    new_model.summary()\n",
    "\n",
    "    gen = ImageDataGenerator() \n",
    "    train_generator = gen.flow_from_directory(\"./data/train\", (width, height), shuffle=True, batch_size=16)\n",
    "    valid_generator = gen.flow_from_directory(\"./data/validation\", (width, height), shuffle=True, batch_size=16)\n",
    "\n",
    "    history = LossHistory()\n",
    "    lrreduce = LearningRateScheduler(lr_scheduler)\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", min_delta=0.0003, patience=5, verbose=1, mode=\"min\")\n",
    "    if tune:\n",
    "        checkpoint = ModelCheckpoint(\"./%s.hdf5\"%MODEL.__name__,monitor=\"val_loss\", mode=\"min\", save_best_only=True, period=1)\n",
    "    else:\n",
    "        checkpoint = ModelCheckpoint(\"./a_%s.hdf5\"%MODEL.__name__,monitor=\"val_loss\", mode=\"min\", save_best_only=True, period=1)\n",
    "   \n",
    "    model = new_model.fit_generator(train_generator, 1+tra_nb//16, Epochs, verbose=1,\n",
    "                                    validation_data=valid_generator, callbacks=[history,earlystopping,checkpoint,lrreduce])\n",
    "    \n",
    "    History[\"%s\"%MODEL.__name__] = model.history\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IncetionResNetV2，选出合适的h5py\n",
    "adam = Adam(lr=0.0001)\n",
    "new_model = Model_fit((299,299),InceptionResNetV2,inception_resnet_v2.preprocess_input,adam,History,Epochs=15,drop=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在adam基础上利用sgd进行精调，降低val_loss，提高val_acc\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "new_model = Model_fit((299,299),InceptionResNetV2,inception_resnet_v2.preprocess_input,sgd,History,tune=1,Epochs=15,drop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据模型预测结果\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"./data/test\", (32, 32),batch_size=16)\n",
    "y_pred = new_model.predict_generator(test_generator, 1+79726//16, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "# 存入\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df = pd.DataFrame(y_pred,df.values[:,0],df.columns[1:])\n",
    "df.to_csv('pred3.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#InceptionV3，选出合适的h5py\n",
    "adam = Adam(lr=0.0003)\n",
    "new_model = Model_fit((299,299),InceptionV3,inception_v3.preprocess_input,adam,History,tune=0,drop=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在adam基础上利用sgd进行精调，降低val_loss，提高val_acc\n",
    "sgd = SGD(lr=0.0005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "new_model = Model_fit((299,299),InceptionV3,inception_v3.preprocess_input,sgd,History,tune=1,drop=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据模型预测结果\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"./data/test\", (299, 299),batch_size=16)\n",
    "y_pred = new_model.predict_generator(test_generator, 1+79726//16, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "# 存入\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df = pd.DataFrame(y_pred,df.values[:,0],df.columns[1:])\n",
    "df.to_csv('pred.csv3')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xception，选出合适的h5py\n",
    "adam = Adam(lr=0.0001)\n",
    "new_model = Model_fit((299,299),Xception,xception.preprocess_input,adam,History,tune=0,drop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在adam基础上利用sgd进行精调，降低val_loss，提高val_acc\n",
    "sgd = SGD(lr=0.00005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "new_model = Model_fit((299,299),Xception,xception.preprocess_input,sgd,History,tune=1,drop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #根据模型预测结果\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"./data/test\", (299, 299),batch_size=16)\n",
    "# y_pred = new_model.predict_generator(test_generator, 1+79726//16, verbose=1)\n",
    "# y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "# print(y_pred[:3])\n",
    "\n",
    "acc = []\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    name = fname[fname.rfind('/')+1:]\n",
    "    acc.append( [name,y_pred[i][0],y_pred[i][1],y_pred[i][2],y_pred[i][3],y_pred[i][4],\n",
    "                 y_pred[i][5],y_pred[i][6],y_pred[i][7],y_pred[i][8],y_pred[i][9]] )\n",
    "\n",
    "print (acc[:10])\n",
    "\n",
    "df = pd.DataFrame(acc,columns=['img'] + ['c%d'%i for i in range(10)])\n",
    "df.head(10)\n",
    "df = df.sort_values(by='img')\n",
    "df.to_csv('pred4.csv',index=None, float_format='%.3f')\n",
    "print(\"csv saved\")\n",
    "# # 存入\n",
    "# df = pd.read_csv('sample_submission.csv')\n",
    "# df = pd.DataFrame(y_pred,df.values[:,0],df.columns[1:])\n",
    "# df.to_csv('pred3.csv')\n",
    "# df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据模型预测结果\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"./data/test\", (32, 32),batch_size=16)\n",
    "y_pred = new_model.predict_generator(test_generator, 1+79726//16, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "# 存入\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df = pd.DataFrame(y_pred,df.values[:,0],df.columns[1:])\n",
    "df.to_csv('pred.csv3')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_loss学习曲线 & val_acc学习曲线\n",
    "for t in [\"InceptionResNetV2\",\"InceptionV3\",\"Xception\"]:\n",
    "    plt.plot(History[t].val_losses)\n",
    "    plt.xlabel('Times')\n",
    "    plt.ylabel('Val_Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(History[t].val_acces)\n",
    "    plt.xlabel('Times')\n",
    "    plt.ylabel('Val_acc')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "得到三种模型下各自放开所有层的训练结果及相应预测csv，关于csv进行后期数据整理（这里用平均求和的方式，并取最大值为测试结果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#平均结果\n",
    "df1 = pd.read_csv('pred1.csv')\n",
    "df2 = pd.read_csv('pred2.csv')\n",
    "df3 = pd.read_csv('pred3.csv')\n",
    "result = (df1+df2+df3)/3\n",
    "result.to_csv('pred.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# df1 = pd.read_csv('pred01.csv')\n",
    "# df2 = pd.read_csv('pred1.csv')\n",
    "# df3 = pd.read_csv('pred11.csv')\n",
    "# DF1 = pd.DataFrame(df1.values[:,1:],df.values[:,0],df.columns[1:])\n",
    "# DF2 = pd.DataFrame(df2.values[:,1:],df.values[:,0],df.columns[1:])\n",
    "# DF3 = pd.DataFrame(df3.values[:,1:],df.values[:,0],df.columns[1:])\n",
    "# result = (DF1+DF2+DF3)/3\n",
    "# print (result)\n",
    "# result.to_csv('pred.csv')\n",
    "\n",
    "# # 存入\n",
    "# df = pd.read_csv('sample_submission.csv')\n",
    "# for i in range(df.shape[0]):\n",
    "#     for j in range(1,len(df.columns)):\n",
    "#         df.set_value(i,df.columns[j], result[i][j-1])\n",
    "# print (df.values[:,1:])\n",
    "# print (df.values[:,1:].shape)\n",
    "# print (df.values[:,0])\n",
    "# print (df.values[:,0].shape)\n",
    "# print (df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Resnet50\n",
    "\n",
    "# width = 224\n",
    "# height = 224\n",
    "# input_tensor = Input((height, width, 3))\n",
    "# x = Lambda(resnet50.preprocess_input)(input_tensor)\n",
    "        \n",
    "# base_model = ResNet50(input_tensor=x, input_shape=(width, height, 3), include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "# new_output = Dropout(0.3)(base_model.output)\n",
    "# new_output = Dense(10, activation=\"softmax\")(new_output)\n",
    "# new_model = Model(inputs=base_model.input, outputs=new_output)\n",
    "    \n",
    "# adam = Adam(lr=0.0001)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# new_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# new_model.summary()\n",
    "\n",
    "# gen = ImageDataGenerator() \n",
    "# train_generator = gen.flow_from_directory(\"./data/train\", (width, height), shuffle=True,batch_size=16)\n",
    "# validation_generator = gen.flow_from_directory(\"./data/validation\", (width, height), shuffle=True,batch_size=16)\n",
    "\n",
    "# history = LossHistory()\n",
    "# earlystopping = EarlyStopping(monitor=\"val_loss\", min_delta=0.0003, patience=5, verbose=1, mode=\"min\")\n",
    "# checkpoint = ModelCheckpoint(\"./ResNet50.hdf5\",monitor=\"val_loss\", mode=\"min\", save_best_only=True,period=1)\n",
    "# model = new_model.fit_generator(train_generator,1+tra_nb//16,epochs=15,verbose=1,\n",
    "#                                 validation_data=validation_generator,callbacks=[history,earlystopping,checkpoint])\n",
    "# History[\"ResNet50\"] = model.history\n",
    "\n",
    "# test_generator = gen.flow_from_directory(\"./data/test\", (width, height),batch_size=16)\n",
    "# y_pred = new_model.predict_generator(test_generator, 1+tex_nb//16)\n",
    "# y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "\n",
    "# # 读入测试集文件夹下所有图片并对图片进行预处理，用于predict\n",
    "# df = pd.read_csv(\"sample_submission.csv\")\n",
    "# for i, fname in enumerate(test_generator.filenames):\n",
    "#     index = int(fname[fname.rfind('_')+1:fname.rfind('.')])\n",
    "#     for j in range(1,len(df.columns)):\n",
    "#         df.set_value(index-1,df.columns[j], y_pred[i][j-1])\n",
    "        \n",
    "# df.to_csv('pred0.csv', index=None)\n",
    "# df.head(10)\n",
    "\n",
    "# # 读入测试集文件夹下所有图片并对图片进行预处理，用于predict\n",
    "# df = pd.read_csv('sample_submission.csv')\n",
    "# for i in range(df.shape[0]):\n",
    "#     for j in range(1,len(df.columns)):\n",
    "#         df.set_value(i,df.columns[j], y_pred[i][j-1])\n",
    "\n",
    "# df.to_csv('pred1.csv', index=None)\n",
    "# df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
